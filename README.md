# ğŸ§  ML Learning Journey â€” Balaji Visvanathan

Welcome to my Machine Learning learning journey!  
This repository documents my step-by-step path to mastering ML â€” from data cleaning to deep learning â€” through 8 structured weeks of coding, math, and projects.

---

## ğŸ¯ Goal
To build a solid foundation in Machine Learning â€” understanding not just *how to use models*, but *how they work*, backed by strong mathematical intuition.  
Iâ€™m an ECE student exploring multiple domains (ML, IoT, Quant, DSA), aiming to become a **jack of all trades â€” and a master of a few**.

---

## â³ Weekly Breakdown (8 Hrs/Week)

### **Week 1 â€” ML Foundations & Data Handling**
- Understand ML workflow (data â†’ model â†’ evaluation)
- Clean and preprocess datasets  
- Build logistic regression on Titanic dataset  
- **Math:** Mean, variance, normalization, correlation  
- **Outcome:** Grasp end-to-end ML flow & metrics

---

### **Week 2 â€” Regression Models**
- Linear Regression, Ridge, Lasso  
- Bias-variance tradeoff, gradient descent basics  
- **Project:** California Housing Price prediction  
- **Math:** Cost function (MSE), partial derivatives  
- **Outcome:** Build & interpret regression models

---

### **Week 3 â€” Classification Models**
- Logistic Regression, k-NN, Decision Trees  
- Precision, recall, F1-score, ROC  
- **Project:** Heart Disease Prediction  
- **Math:** Sigmoid, entropy, information gain  
- **Outcome:** Train and compare classifiers

---

### **Week 4 â€” Feature Engineering + Pipelines**
- One-hot encoding, scaling, feature selection  
- Building clean preprocessing pipelines  
- **Project:** Bank Marketing Dataset  
- **Math:** Variance, feature importance  
- **Outcome:** Reusable pipelines & improved model accuracy

---

### **Week 5 â€” Ensemble Learning**
- Bagging, Boosting, Random Forest, XGBoost  
- **Project:** Titanic Revisited â€” Ensemble comparison  
- **Math:** Bias-variance intuition  
- **Outcome:** Learn to combine models for better results

---

### **Week 6 â€” Unsupervised Learning**
- K-Means, Hierarchical Clustering, PCA  
- **Project:** Iris / Mall Customers clustering  
- **Math:** Distance metrics, eigenvalues, covariance matrix  
- **Outcome:** Discover patterns without labels

---

### **Week 7 â€” Neural Networks & Deep Learning Intro**
- Perceptron, Feedforward Network, Activation Functions  
- **Project:** MNIST Handwritten Digits (NumPy + Keras)  
- **Math:** Weighted sums, ReLU, softmax, cross-entropy  
- **Outcome:** First neural network from scratch

---

### **Week 8 â€” Capstone Project**
- End-to-end ML project (IoT, Finance, or Healthcare dataset)  
- Complete documentation, model evaluation, and presentation  
- **Math:** Confidence intervals, error analysis  
- **Outcome:** Portfolio-ready ML project on GitHub

---

## ğŸ§© Supporting Learning (Ongoing)
- **Linear Algebra:** 3Blue1Brown â€“ *Essence of Linear Algebra*  
- **Probability & Statistics:** Blitzstein & Hwang + *StatQuest* YouTube  
- **Calculus:** 3Blue1Brown â€“ *Essence of Calculus*

---

## ğŸ§  Tools & Environment
- **Language:** Python 3.x  
- **Libraries:** NumPy, Pandas, Matplotlib, scikit-learn, TensorFlow/Keras  
- **Environment:** Ubuntu + Jupyter Lab  
- **Version Control:** Git & GitHub (SSH setup complete âœ…)

---

## ğŸ§­ Current Status
- âœ… Week 1: Titanic dataset â€” baseline model complete  
- ğŸ”„ Week 2: Regression Models â€” in progress  
- ğŸ“… Next Goal: House price prediction using California dataset  

---

### ğŸ“Œ Notes
This repo will evolve as I learn â€” with each notebook, concept, and project clearly documented.  
Stay tuned for updates, reflections, and model improvements!

---

**Â© 2025 Balaji Visvanathan â€” Machine Learning Journey**
